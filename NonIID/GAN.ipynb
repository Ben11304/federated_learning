{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_work import processed\n",
    "df=pd.read_csv(\"/Users/mac/Dev/data/dta_IoT/new2test.csv\")\n",
    "#delete all null columns\n",
    "id=[16,17,21,22,23,24]\n",
    "col=df.columns\n",
    "for idx in id:\n",
    "    df=df.drop(col[idx],axis=1)\n",
    "data=processed(df,\"subcategory \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subcategory</th>\n",
       "      <th>pkSeqID</th>\n",
       "      <th>stime</th>\n",
       "      <th>flgs</th>\n",
       "      <th>proto</th>\n",
       "      <th>saddr</th>\n",
       "      <th>sport</th>\n",
       "      <th>daddr</th>\n",
       "      <th>dport</th>\n",
       "      <th>pkts</th>\n",
       "      <th>...</th>\n",
       "      <th>max</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>srate</th>\n",
       "      <th>drate</th>\n",
       "      <th>attack</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3711</td>\n",
       "      <td>6284</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4415</td>\n",
       "      <td>17</td>\n",
       "      <td>600</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1516</td>\n",
       "      <td>1428</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3130</td>\n",
       "      <td>44</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1151</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>86</td>\n",
       "      <td>1534</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>6150</td>\n",
       "      <td>3656</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>432</td>\n",
       "      <td>25</td>\n",
       "      <td>901</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>2171</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>368</td>\n",
       "      <td>552</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2083</td>\n",
       "      <td>1903</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3079</td>\n",
       "      <td>20</td>\n",
       "      <td>847</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>191</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2279</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1677</td>\n",
       "      <td>1588</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3278</td>\n",
       "      <td>27</td>\n",
       "      <td>697</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>525</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7021</th>\n",
       "      <td>6</td>\n",
       "      <td>5814</td>\n",
       "      <td>3323</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2679</td>\n",
       "      <td>25</td>\n",
       "      <td>901</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>424</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7022</th>\n",
       "      <td>5</td>\n",
       "      <td>713</td>\n",
       "      <td>694</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1908</td>\n",
       "      <td>20</td>\n",
       "      <td>948</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2456</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7023</th>\n",
       "      <td>0</td>\n",
       "      <td>3961</td>\n",
       "      <td>5427</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1966</td>\n",
       "      <td>20</td>\n",
       "      <td>372</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>2391</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>462</td>\n",
       "      <td>130</td>\n",
       "      <td>1611</td>\n",
       "      <td>1288</td>\n",
       "      <td>1062</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7024</th>\n",
       "      <td>3</td>\n",
       "      <td>1872</td>\n",
       "      <td>1782</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1883</td>\n",
       "      <td>20</td>\n",
       "      <td>731</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>2531</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>538</td>\n",
       "      <td>0</td>\n",
       "      <td>1694</td>\n",
       "      <td>1428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7025</th>\n",
       "      <td>2</td>\n",
       "      <td>3827</td>\n",
       "      <td>6400</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2783</td>\n",
       "      <td>24</td>\n",
       "      <td>609</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>794</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>507</td>\n",
       "      <td>32</td>\n",
       "      <td>2014</td>\n",
       "      <td>1498</td>\n",
       "      <td>1247</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7026 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subcategory   pkSeqID  stime  flgs  proto  saddr  sport  daddr  dport  \\\n",
       "0                2     3711   6284     0      2      5   4415     17    600   \n",
       "1                3     1516   1428     0      3      5   3130     44    697   \n",
       "2                7     6150   3656     0      3      0    432     25    901   \n",
       "3                4     2083   1903     0      2      3   3079     20    847   \n",
       "4                3     1677   1588     0      3      3   3278     27    697   \n",
       "...            ...      ...    ...   ...    ...    ...    ...    ...    ...   \n",
       "7021             6     5814   3323     7      2      2   2679     25    901   \n",
       "7022             5      713    694     0      2      3   1908     20    948   \n",
       "7023             0     3961   5427     0      2      3   1966     20    372   \n",
       "7024             3     1872   1782     0      3     11   1883     20    731   \n",
       "7025             2     3827   6400     0      2      2   2783     24    609   \n",
       "\n",
       "      pkts  ...   max  spkts  dpkts  sbytes  dbytes  rate  srate  drate  \\\n",
       "0        1  ...   130      0      1       2       1  2348      0      0   \n",
       "1        1  ...  1151      0      1       7      86  1534      0      0   \n",
       "2        9  ...  2171      9      0      71       0   368    552      0   \n",
       "3        1  ...   191      0      1       0       1  2279      0      0   \n",
       "4        1  ...     0      1      0      41       0   525    710      0   \n",
       "...    ...  ...   ...    ...    ...     ...     ...   ...    ...    ...   \n",
       "7021     6  ...     0      6      0     424       0   104     34      0   \n",
       "7022     1  ...    46      0      1       0       1  2456      0      0   \n",
       "7023    27  ...  2391     14     15     462     130  1611   1288   1062   \n",
       "7024    65  ...  2531     51      0     538       0  1694   1428      0   \n",
       "7025     9  ...   794      5      4     507      32  2014   1498   1247   \n",
       "\n",
       "      attack  category  \n",
       "0          1         3  \n",
       "1          0         1  \n",
       "2          1         0  \n",
       "3          1         2  \n",
       "4          0         1  \n",
       "...      ...       ...  \n",
       "7021       1         0  \n",
       "7022       1         2  \n",
       "7023       1         3  \n",
       "7024       0         1  \n",
       "7025       1         3  \n",
       "\n",
       "[7026 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data=torch.tensor(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = torch.sum(labels_one_hot, dim=0)\n",
    "num_labels = label_counts.size(0)\n",
    "for label_idx in range(num_labels):\n",
    "    label_count = label_counts[label_idx].item()\n",
    "    print(f\"Label {label_idx}: {label_count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=torch.tensor(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=nn.Embedding(8,8)\n",
    "y=data[:32,:1]\n",
    "x=data[:32,1:]\n",
    "print(x.size())\n",
    "print(y.size())\n",
    "tes=torch.cat((embed(y).squeeze(),x),1)\n",
    "tes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_encode(labels, num_classes):\n",
    "    num_samples = len(labels)\n",
    "    encoded_labels = np.zeros((num_samples, num_classes))\n",
    "    for i in range(num_samples):\n",
    "        label = labels[i]\n",
    "        encoded_labels[i, label] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils, torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(generator, self).__init__()\n",
    "        self.input_dim = args.noise_size\n",
    "        self.output_dim = args.n_features\n",
    "        self.class_num = args.n_classes\n",
    "        self.label_emb = nn.Embedding(self.class_num,self.class_num)\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "    \n",
    "        self.model = nn.Sequential(\n",
    "            *block(self.input_dim + self.class_num, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024,self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, noise ,label):\n",
    "        x = torch.cat((self.label_emb(label).squeeze(),noise), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.input_dim = args.n_features\n",
    "        self.output_dim = args.n_features\n",
    "        self.class_num = args.n_classes\n",
    "        self.label_emb = nn.Embedding(self.class_num,self.class_num)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear((self.class_num + self.input_dim), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(256, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(256, self.class_num), nn.Softmax())\n",
    "        \n",
    "\n",
    "    def forward(self, input ,label):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        x = torch.cat((self.label_emb(label).squeeze(), input), 1)\n",
    "        x=self.model(x)\n",
    "        real=self.adv_layer(x)\n",
    "        label=self.aux_layer(x)\n",
    "        return real,label \n",
    "    \n",
    "class CGAN(object):\n",
    "    def __init__(self, args):\n",
    "        # parameters\n",
    "        self.epoch = args.epoch\n",
    "        self.batch_size = args.batch_size\n",
    "        self.save_dir = args.save_dir\n",
    "        self.result_dir = args.result_dir\n",
    "        self.dataset = args.dataset\n",
    "        self.log_dir = args.log_dir\n",
    "        self.gpu_mode = args.gpu_mode\n",
    "        self.model_name = args.gan_type\n",
    "        self.z_dim = args.z_dim\n",
    "        self.class_num = args.n_class\n",
    "        # load dataset\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        data=next(iter(self.data_loader))\n",
    "        #option\n",
    "        dim=int(data.shape[1])-8\n",
    "\n",
    "        # networks init\n",
    "        self.G = generator(args)\n",
    "        self.D = discriminator(args)\n",
    "        self.G_optimizer = optim.RMSprop(self.G.parameters(), lr=args.lrG, alpha=0.9)\n",
    "        self.D_optimizer = optim.RMSprop(self.D.parameters(), lr=args.lrD, alpha=0.9)\n",
    "        \n",
    "        self.MSE_loss = torch.nn.MSELoss()\n",
    "        self.BCE_loss=nn.BCELoss()\n",
    "        # Loss functions\n",
    "        self.adversarial_loss = torch.nn.BCELoss()\n",
    "        self.auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
    "        print('---------- Networks architecture -------------')\n",
    "        utils.print_network(self.G)\n",
    "        utils.print_network(self.D)\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_loss'] = []\n",
    "        self.train_hist['G_loss'] = []\n",
    "        self.train_hist['per_epoch_time'] = []\n",
    "        self.train_hist['total_time'] = []\n",
    "\n",
    "        self.y_real_ = Variable(torch.FloatTensor(self.batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        self.y_fake_ = Variable(torch.FloatTensor(self.batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        #self.y_real_, self.y_fake_ = torch.ones(self.batch_size, 1), torch.zeros(self.batch_size, 1)\n",
    "\n",
    "\n",
    "\n",
    "        self.D.train()\n",
    "        print('training start!!')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epoch):\n",
    "            self.G.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for iter, da in enumerate(self.data_loader):\n",
    "                x_=da[:,1:].float()\n",
    "                y_=da[:,:1].int()\n",
    "                if iter == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    break\n",
    "\n",
    "                z_ = Variable(torch.FloatTensor(np.random.normal(0, 1, (self.batch_size, self.z_dim))))\n",
    "                #y_check= onehot_encode(y_,8)\n",
    "                #y_check=torch.tensor(y_check)\n",
    "                y_check=y_\n",
    "                # update D network\n",
    "                self.D.train()\n",
    "                self.G.eval()\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "\n",
    "                real,label = self.D(x_, y_)\n",
    "                #D_real_loss = self.MSE_loss(D_real, self.y_real_)\n",
    "                G_ = self.G(z_, y_)\n",
    "                fake,label_ = self.D(G_, y_)\n",
    "                #D_fake_loss = self.MSE_loss(D_fake, self.y_fake_)\n",
    "                D_fake_loss=(self.adversarial_loss(fake, self.y_fake_) + self.auxiliary_loss(label_, y_check)) / 2\n",
    "                D_real_loss=(self.adversarial_loss(real, self.y_real_) + self.auxiliary_loss(label, y_check)) / 2\n",
    "\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "                self.train_hist['D_loss'].append(D_loss.item())\n",
    "\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "\n",
    "                 # update G network\n",
    "                self.D.eval()\n",
    "                self.G.train()\n",
    "                self.G_optimizer.zero_grad()\n",
    "\n",
    "                z_=torch.rand((self.batch_size, self.z_dim))\n",
    "                G_ = self.G(z_, y_)\n",
    "                real,label = self.D(G_, y_)\n",
    "                #G_loss = self.MSE_loss(D_fake, self.y_real_)\n",
    "\n",
    "                G_loss=0.5 * (self.adversarial_loss(real, self.y_real_) + self.auxiliary_loss(label, y_check))\n",
    "                self.train_hist['G_loss'].append(G_loss.item())\n",
    "\n",
    "                G_loss.backward()\n",
    "                self.G_optimizer.step()\n",
    "\n",
    "               \n",
    "                if (iter + 1) == self.data_loader.dataset.__len__() // self.batch_size:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f\" %\n",
    "                          ((epoch + 1), (iter + 1), self.data_loader.dataset.__len__() // self.batch_size, D_loss.item(), G_loss.item()))\n",
    "\n",
    "            self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)\n",
    "                \n",
    "\n",
    "        self.train_hist['total_time'].append(time.time() - start_time)\n",
    "        print(\"Avg one epoch time: %.2f, total %d epochs time: %.2f\" % (np.mean(self.train_hist['per_epoch_time']),\n",
    "              self.epoch, self.train_hist['total_time'][0]))\n",
    "        print(\"Training finish!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self,data):\n",
    "        self.epoch = 10\n",
    "        self.batch_size = 32\n",
    "        self.save_dir = '/Users/mac/Dev/source/article/FerderatedLearning/federated_learning/NonIID/model.h5'\n",
    "        self.result_dir = '/Users/mac/Dev/source/article/FerderatedLearning/federated_learning/NonIID/result.txt'\n",
    "        self.dataset = data\n",
    "        self.log_dir = 'logs/'\n",
    "        self.gpu_mode = True\n",
    "        self.gan_type = 'cGAN'\n",
    "        self.z_dim = 10\n",
    "        self.n_class = 8\n",
    "        self.sample_num = self.n_class ** 2\n",
    "        self.lrG=0.0001\n",
    "        self.lrD=0.0001\n",
    "        self.n_epochs=self.n_class\n",
    "        self.n_classes=self.n_class\n",
    "        self.noise_size=self.z_dim\n",
    "        self.n_features=28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 3711, 6284,  ...,    0,    1,    3],\n",
      "        [   3, 1516, 1428,  ...,    0,    0,    1],\n",
      "        [   7, 6150, 3656,  ...,    0,    1,    0],\n",
      "        ...,\n",
      "        [   0, 3961, 5427,  ..., 1062,    1,    3],\n",
      "        [   3, 1872, 1782,  ...,    0,    0,    1],\n",
      "        [   2, 3827, 6400,  ..., 1247,    1,    3]])\n"
     ]
    }
   ],
   "source": [
    "arg=Args(data)\n",
    "print(arg.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Networks architecture -------------\n",
      "generator(\n",
      "  (label_emb): Embedding(8, 8)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=128, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Linear(in_features=1024, out_features=28, bias=True)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 724700\n",
      "discriminator(\n",
      "  (label_emb): Embedding(8, 8)\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=36, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (6): Dropout(p=0.4, inplace=False)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "  )\n",
      "  (adv_layer): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      "  (aux_layer): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=8, bias=True)\n",
      "    (1): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "Total number of parameters: 677961\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model=CGAN(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 145\u001b[0m, in \u001b[0;36mCGAN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m fake,label_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD(G_, y_)\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#D_fake_loss = self.MSE_loss(D_fake, self.y_fake_)\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m D_fake_loss\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madversarial_loss(fake, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_fake_) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauxiliary_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_check\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    146\u001b[0m D_real_loss\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madversarial_loss(real, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_real_) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauxiliary_loss(label, y_check)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    149\u001b[0m D_loss \u001b[38;5;241m=\u001b[39m D_real_loss \u001b[38;5;241m+\u001b[39m D_fake_loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=torch.tensor([[1],[2],[3],[4],[5],[6],[7],[0],[1],[2],[3],[4],[5],[6],[7],[0],[1],[2],[3],[4],[5],[6],[7],[0]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len=y.shape[0]\n",
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0j/2h4f0bj12r775fg_s2v4hpqh0000gn/T/ipykernel_89047/1650534317.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(y)\n"
     ]
    }
   ],
   "source": [
    "z= torch.rand(len,10)\n",
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm=torch.cat([z,y],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.2243e-01, 1.0103e-01, 2.1562e-01, 6.5246e-01, 2.4079e-01, 8.1744e-01,\n",
       "         3.4712e-01, 3.8573e-01, 7.0011e-01, 6.8769e-01, 1.0000e+00],\n",
       "        [1.3400e-01, 6.7474e-01, 6.0900e-01, 6.1009e-01, 8.1475e-01, 9.4652e-01,\n",
       "         9.5120e-01, 9.6646e-01, 6.2355e-01, 8.9124e-01, 2.0000e+00],\n",
       "        [5.2764e-01, 9.7205e-01, 8.7093e-01, 5.4650e-01, 3.7128e-01, 5.0396e-01,\n",
       "         9.7573e-01, 7.4810e-01, 1.0209e-01, 4.4495e-02, 3.0000e+00],\n",
       "        [1.5447e-01, 5.1976e-01, 4.6833e-01, 7.2069e-01, 2.5893e-01, 2.8095e-01,\n",
       "         5.8347e-01, 5.3661e-01, 9.7970e-01, 1.8402e-01, 4.0000e+00],\n",
       "        [2.9829e-02, 6.3900e-01, 1.1078e-01, 7.5116e-03, 6.1060e-02, 7.6196e-01,\n",
       "         1.6047e-01, 6.0523e-01, 9.1689e-01, 7.0519e-01, 5.0000e+00],\n",
       "        [6.2035e-01, 5.1903e-01, 7.1086e-01, 5.8471e-01, 3.1769e-01, 3.1155e-01,\n",
       "         8.8932e-01, 5.8373e-03, 2.0105e-01, 7.6207e-01, 6.0000e+00],\n",
       "        [1.8573e-01, 9.0780e-01, 8.0976e-01, 4.6425e-01, 2.7944e-01, 2.1434e-01,\n",
       "         1.7934e-01, 3.3511e-01, 1.3206e-01, 8.7422e-01, 7.0000e+00],\n",
       "        [5.6266e-01, 5.5865e-01, 6.1372e-01, 7.7483e-01, 3.8571e-01, 5.1046e-01,\n",
       "         8.9273e-01, 2.7521e-01, 1.5643e-01, 8.9307e-01, 0.0000e+00],\n",
       "        [2.1083e-01, 7.1381e-01, 3.4406e-01, 5.1826e-01, 5.4136e-01, 4.8976e-02,\n",
       "         1.2890e-01, 1.6460e-03, 7.5401e-01, 8.6427e-01, 1.0000e+00],\n",
       "        [1.8075e-02, 3.5766e-01, 7.5075e-01, 9.9459e-01, 9.0752e-01, 3.6312e-01,\n",
       "         7.9353e-01, 1.2323e-01, 1.8067e-01, 7.0068e-02, 2.0000e+00],\n",
       "        [9.3438e-01, 3.5085e-01, 3.0718e-01, 2.4231e-01, 1.3578e-01, 9.7078e-01,\n",
       "         2.4068e-01, 5.0597e-01, 4.2760e-01, 5.9309e-02, 3.0000e+00],\n",
       "        [8.6485e-01, 7.0798e-01, 2.5776e-01, 3.0184e-01, 5.0901e-02, 7.8538e-01,\n",
       "         1.3686e-01, 2.2480e-01, 3.3270e-02, 7.5896e-01, 4.0000e+00],\n",
       "        [1.9073e-01, 1.4449e-01, 9.5187e-01, 8.7433e-01, 7.3118e-01, 3.7594e-01,\n",
       "         4.3772e-01, 2.2852e-01, 3.0861e-01, 8.1212e-01, 5.0000e+00],\n",
       "        [7.4822e-01, 4.0734e-01, 5.7599e-01, 4.8251e-01, 5.7515e-01, 7.7493e-01,\n",
       "         8.4943e-01, 7.0503e-01, 8.3958e-01, 7.6592e-01, 6.0000e+00],\n",
       "        [4.7815e-01, 3.0166e-01, 8.7207e-02, 4.9701e-01, 2.3703e-01, 3.6558e-02,\n",
       "         6.8889e-01, 3.7626e-01, 6.9267e-01, 5.7417e-01, 7.0000e+00],\n",
       "        [5.0973e-02, 4.9736e-01, 7.5530e-01, 5.5246e-01, 2.9414e-02, 8.1644e-01,\n",
       "         8.2137e-01, 1.9687e-01, 6.0932e-01, 3.7358e-01, 0.0000e+00],\n",
       "        [6.9952e-01, 4.8250e-01, 4.4423e-01, 9.3612e-01, 8.4375e-01, 6.0306e-01,\n",
       "         2.2666e-01, 2.2854e-01, 9.3837e-01, 3.6795e-01, 1.0000e+00],\n",
       "        [2.8803e-01, 8.8059e-01, 3.0937e-01, 9.6144e-01, 1.1506e-01, 3.0427e-01,\n",
       "         8.5390e-01, 7.9576e-02, 4.8220e-01, 3.7502e-01, 2.0000e+00],\n",
       "        [5.2930e-02, 1.3446e-01, 4.2441e-01, 1.5896e-01, 8.5796e-01, 9.1096e-01,\n",
       "         3.6770e-01, 9.2271e-01, 8.3892e-01, 5.2104e-01, 3.0000e+00],\n",
       "        [9.8867e-01, 8.3384e-01, 7.8986e-01, 7.0015e-01, 1.1720e-01, 1.3445e-01,\n",
       "         1.9839e-01, 1.9498e-01, 3.6075e-01, 6.5909e-01, 4.0000e+00],\n",
       "        [6.2362e-01, 6.0496e-01, 1.8553e-01, 3.4367e-01, 1.6809e-01, 1.5885e-01,\n",
       "         9.8343e-01, 8.1980e-02, 5.8908e-01, 8.7801e-01, 5.0000e+00],\n",
       "        [2.8461e-01, 4.3993e-01, 7.0998e-01, 3.4708e-01, 7.9245e-01, 5.8478e-01,\n",
       "         1.2202e-02, 1.9441e-01, 1.6512e-01, 1.7807e-01, 6.0000e+00],\n",
       "        [6.8384e-01, 6.8633e-01, 4.7844e-01, 8.4023e-01, 6.3498e-01, 2.5713e-01,\n",
       "         4.6391e-01, 6.5342e-01, 1.9472e-01, 1.3386e-01, 7.0000e+00],\n",
       "        [3.0903e-01, 7.2151e-01, 7.5391e-01, 1.5730e-02, 9.2616e-01, 8.3228e-01,\n",
       "         8.9661e-01, 9.0012e-01, 8.9434e-02, 3.0754e-01, 0.0000e+00]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fake=model.G(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.7018e+01,  1.0032e+02,  1.7561e+00,  4.3638e-02,  2.5480e+00,\n",
       "          3.1089e+01,  1.8573e+00,  1.0897e+01,  3.0790e+00,  3.4829e+00,\n",
       "         -1.7003e+00,  9.4960e+01,  1.5222e+01,  3.4034e+01,  2.5746e+01,\n",
       "          3.4629e-01,  1.4285e+01,  8.5204e+00,  2.1589e+01, -5.2481e+00,\n",
       "          3.0474e+00,  2.1635e+00,  4.4235e+00,  2.6075e+01,  1.4781e+01,\n",
       "          4.8550e+00, -9.5278e-01,  5.2621e+00],\n",
       "        [ 6.2846e+01,  4.7878e+01,  8.5074e-01,  1.8941e-02, -2.4761e+00,\n",
       "          1.0175e+02,  3.2814e+00,  3.1476e+01, -1.3504e+00,  7.9025e+00,\n",
       "          2.3773e+00,  5.7999e+01,  3.4034e+01,  1.3652e+01,  1.8854e-01,\n",
       "         -5.9643e+00,  5.9182e+00,  1.1616e+01,  6.3129e+00, -1.3573e-01,\n",
       "          1.1876e+00,  8.9249e-01, -1.0012e+00,  8.7436e+01, -8.2863e-01,\n",
       "          2.1164e+00, -2.9009e+00, -3.3203e+00],\n",
       "        [ 2.4500e+01,  1.3391e+01, -1.5957e-01,  1.3286e+00, -4.6580e-01,\n",
       "          4.4691e+01,  9.9341e-01,  1.7558e+01,  1.2243e+00, -1.0922e+00,\n",
       "          3.1595e+00,  1.7702e+01,  2.7227e+01,  1.8205e+01,  2.7949e+00,\n",
       "         -1.5012e+00,  9.0597e+00,  5.7441e+00,  5.5654e+00,  4.6335e-01,\n",
       "         -2.0038e+00,  1.8954e+00,  3.8776e-01,  2.2845e+01,  2.0959e+00,\n",
       "          2.9037e+00, -1.3254e+00,  2.4033e+00],\n",
       "        [ 9.0000e+01,  9.1665e+01,  1.4473e+00,  7.5356e-01, -4.9929e+00,\n",
       "          8.6914e+01,  3.0289e+00,  1.9337e+01,  7.0953e-01,  1.0153e+01,\n",
       "         -7.7460e-01,  9.7339e+01,  2.7792e+01,  1.7313e+01,  4.0656e+00,\n",
       "         -4.1111e+00, -2.3816e+00,  9.6863e+00,  2.3273e+00, -2.6932e+00,\n",
       "          3.1067e+00,  2.1969e+00, -1.9430e+00,  7.2804e+01,  5.1273e+00,\n",
       "          4.3907e+00, -6.3362e+00, -8.1589e+00],\n",
       "        [ 4.7468e+01,  2.2928e+01, -2.1541e-01, -1.3401e+00,  4.7808e-01,\n",
       "          1.0653e+02,  4.1311e+00,  3.9927e+01, -2.0128e+00,  5.7084e+00,\n",
       "          4.2744e+00,  3.2821e+01,  3.6749e+01,  1.4960e+01,  1.8081e+00,\n",
       "         -6.1250e+00,  1.4809e+01,  1.2864e+01,  1.2363e+01,  6.8753e-01,\n",
       "         -4.3981e-01,  2.5974e-01,  6.3278e-01,  9.1952e+01, -2.7186e+00,\n",
       "          1.4308e+00, -1.6071e+00,  1.6348e+00],\n",
       "        [ 9.2315e+01,  5.9685e+01,  5.6313e-03,  9.3874e-01, -1.3378e+00,\n",
       "          4.0448e+01,  2.1642e-01,  1.6630e+01, -2.5199e-01,  1.4025e+00,\n",
       "          4.6666e+00,  6.3170e+01,  5.6750e+01,  4.6521e+01,  6.8003e+00,\n",
       "         -1.8720e+00,  4.9569e+00, -3.0743e+00, -2.1258e-01,  5.0527e+00,\n",
       "          5.7007e-01,  3.0505e+00, -6.9338e-01,  5.7452e+00,  5.7765e+00,\n",
       "         -3.1397e+00, -8.6252e-01, -3.6976e+00],\n",
       "        [ 9.8550e+01,  6.8698e+01,  1.1170e-01, -1.0166e-01,  9.9283e-01,\n",
       "          2.7869e+01,  4.5139e-01,  1.3990e+01, -1.2591e+00,  1.9134e+00,\n",
       "          2.5407e+00,  6.8282e+01,  4.1158e+01,  4.2632e+01,  1.4286e+01,\n",
       "         -1.6730e+00,  9.1548e+00, -2.4835e+00,  8.9928e+00,  3.4428e+00,\n",
       "          2.5857e+00,  1.9946e+00,  1.1475e+00,  9.9979e+00,  8.2858e+00,\n",
       "         -3.6714e+00,  2.1828e+00, -6.0758e-01],\n",
       "        [ 8.6456e+01,  7.2946e+01,  1.1291e+00,  3.1223e-01,  2.4052e+00,\n",
       "          2.3099e+01,  5.3027e-01,  1.1204e+01,  8.6765e-01,  1.3587e+00,\n",
       "          8.6688e-01,  7.0408e+01,  2.5432e+01,  3.5736e+01,  1.9188e+01,\n",
       "         -4.8006e-01,  1.2613e+01,  3.2348e+00,  1.5881e+01, -7.9069e-01,\n",
       "          2.5640e+00,  1.7565e+00,  3.0948e+00,  1.3241e+01,  1.0556e+01,\n",
       "          3.6269e-01,  2.0890e+00,  3.7847e+00],\n",
       "        [ 9.6039e+01,  9.9151e+01,  1.7521e+00,  6.0241e-02,  2.5561e+00,\n",
       "          3.0757e+01,  1.8189e+00,  1.0846e+01,  3.0486e+00,  3.3923e+00,\n",
       "         -1.6381e+00,  9.3876e+01,  1.5240e+01,  3.3841e+01,  2.5518e+01,\n",
       "          3.3580e-01,  1.4238e+01,  8.4561e+00,  2.1436e+01, -5.1720e+00,\n",
       "          3.0028e+00,  2.1414e+00,  4.4020e+00,  2.5671e+01,  1.4628e+01,\n",
       "          4.7881e+00, -8.8088e-01,  5.2811e+00],\n",
       "        [ 6.1890e+01,  4.6377e+01,  8.1873e-01, -5.4675e-02, -2.3245e+00,\n",
       "          1.0175e+02,  3.3140e+00,  3.1813e+01, -1.4224e+00,  7.8131e+00,\n",
       "          2.4710e+00,  5.6527e+01,  3.4032e+01,  1.3528e+01,  2.0511e-01,\n",
       "         -5.9949e+00,  6.2964e+00,  1.1641e+01,  6.5578e+00, -7.6880e-02,\n",
       "          1.1367e+00,  8.3298e-01, -9.2600e-01,  8.7679e+01, -9.8727e-01,\n",
       "          2.0152e+00, -2.7704e+00, -3.0996e+00],\n",
       "        [ 2.5142e+01,  1.3516e+01, -1.8695e-01,  1.3384e+00, -5.0127e-01,\n",
       "          4.5343e+01,  1.0046e+00,  1.7822e+01,  1.2177e+00, -1.0855e+00,\n",
       "          3.2195e+00,  1.7909e+01,  2.7990e+01,  1.8642e+01,  2.7121e+00,\n",
       "         -1.5294e+00,  9.0803e+00,  5.6720e+00,  5.4230e+00,  5.6528e-01,\n",
       "         -2.0468e+00,  1.9605e+00,  3.5437e-01,  2.2930e+01,  2.0719e+00,\n",
       "          2.8536e+00, -1.3743e+00,  2.3159e+00],\n",
       "        [ 8.9270e+01,  9.1286e+01,  1.4276e+00,  7.5720e-01, -4.9782e+00,\n",
       "          8.5552e+01,  2.9974e+00,  1.8896e+01,  7.6805e-01,  1.0030e+01,\n",
       "         -8.0789e-01,  9.6766e+01,  2.7389e+01,  1.7234e+01,  4.1078e+00,\n",
       "         -4.0097e+00, -2.4625e+00,  9.5454e+00,  2.2479e+00, -2.7016e+00,\n",
       "          3.0748e+00,  2.1980e+00, -1.9435e+00,  7.1502e+01,  5.2167e+00,\n",
       "          4.4082e+00, -6.3415e+00, -8.1108e+00],\n",
       "        [ 4.8066e+01,  2.3752e+01, -1.5994e-01, -1.3230e+00,  4.0910e-01,\n",
       "          1.0674e+02,  4.1271e+00,  3.9797e+01, -2.0352e+00,  5.8598e+00,\n",
       "          4.2148e+00,  3.3715e+01,  3.6606e+01,  1.4788e+01,  1.7624e+00,\n",
       "         -6.1629e+00,  1.4569e+01,  1.2906e+01,  1.2249e+01,  6.3833e-01,\n",
       "         -3.5569e-01,  2.3116e-01,  5.9382e-01,  9.2465e+01, -2.7265e+00,\n",
       "          1.4140e+00, -1.6076e+00,  1.4812e+00],\n",
       "        [ 9.2345e+01,  5.9561e+01, -6.7230e-03,  9.5841e-01, -1.4026e+00,\n",
       "          4.0848e+01,  2.2567e-01,  1.6731e+01, -2.3709e-01,  1.4125e+00,\n",
       "          4.7250e+00,  6.3149e+01,  5.7255e+01,  4.6689e+01,  6.6030e+00,\n",
       "         -1.8771e+00,  4.8293e+00, -3.1185e+00, -4.8913e-01,  5.1087e+00,\n",
       "          5.2116e-01,  3.0836e+00, -7.4969e-01,  5.6798e+00,  5.7055e+00,\n",
       "         -3.1398e+00, -9.6163e-01, -3.8153e+00],\n",
       "        [ 1.0296e+02,  7.2241e+01,  1.2831e-01, -1.8956e-01,  1.2098e+00,\n",
       "          2.8005e+01,  5.1094e-01,  1.4346e+01, -1.3784e+00,  2.0518e+00,\n",
       "          2.4637e+00,  7.1483e+01,  4.1514e+01,  4.4015e+01,  1.5467e+01,\n",
       "         -1.6841e+00,  9.8472e+00, -2.5389e+00,  1.0049e+01,  3.4224e+00,\n",
       "          2.8451e+00,  2.0239e+00,  1.3340e+00,  1.0748e+01,  8.8298e+00,\n",
       "         -3.8286e+00,  2.4511e+00, -4.0871e-01],\n",
       "        [ 8.5117e+01,  7.1438e+01,  1.1520e+00,  4.1571e-01,  2.3113e+00,\n",
       "          2.3140e+01,  4.5252e-01,  1.1200e+01,  8.9778e-01,  1.2166e+00,\n",
       "          9.7768e-01,  6.9133e+01,  2.5825e+01,  3.5614e+01,  1.8766e+01,\n",
       "         -5.0762e-01,  1.2489e+01,  3.2096e+00,  1.5543e+01, -6.9059e-01,\n",
       "          2.4494e+00,  1.7622e+00,  3.0145e+00,  1.2719e+01,  1.0367e+01,\n",
       "          3.4943e-01,  2.1118e+00,  3.7619e+00],\n",
       "        [ 9.9138e+01,  1.0284e+02,  1.7928e+00,  9.4810e-03,  2.6076e+00,\n",
       "          3.1812e+01,  1.9511e+00,  1.1101e+01,  3.1549e+00,  3.6345e+00,\n",
       "         -1.8292e+00,  9.7286e+01,  1.5173e+01,  3.4565e+01,  2.6398e+01,\n",
       "          3.7446e-01,  1.4582e+01,  8.7645e+00,  2.2123e+01, -5.4471e+00,\n",
       "          3.1269e+00,  2.2124e+00,  4.5349e+00,  2.6982e+01,  1.5151e+01,\n",
       "          5.0216e+00, -1.0483e+00,  5.3635e+00],\n",
       "        [ 6.2529e+01,  4.7491e+01,  8.4110e-01, -9.4340e-03, -2.4359e+00,\n",
       "          1.0138e+02,  3.2811e+00,  3.1412e+01, -1.3693e+00,  7.8788e+00,\n",
       "          2.3874e+00,  5.7590e+01,  3.3902e+01,  1.3552e+01,  1.7975e-01,\n",
       "         -5.9615e+00,  5.9332e+00,  1.1567e+01,  6.3130e+00, -1.2062e-01,\n",
       "          1.1909e+00,  8.6302e-01, -9.8483e-01,  8.7224e+01, -8.6709e-01,\n",
       "          2.0687e+00, -2.8595e+00, -3.2885e+00],\n",
       "        [ 2.4705e+01,  1.3421e+01, -1.6700e-01,  1.3415e+00, -4.6633e-01,\n",
       "          4.5077e+01,  9.9411e-01,  1.7754e+01,  1.2328e+00, -1.1191e+00,\n",
       "          3.1883e+00,  1.7742e+01,  2.7476e+01,  1.8405e+01,  2.8518e+00,\n",
       "         -1.5104e+00,  9.2073e+00,  5.7760e+00,  5.6791e+00,  4.7896e-01,\n",
       "         -2.0210e+00,  1.9208e+00,  3.9576e-01,  2.3046e+01,  2.1276e+00,\n",
       "          2.9207e+00, -1.3174e+00,  2.4514e+00],\n",
       "        [ 9.0429e+01,  9.2488e+01,  1.4392e+00,  7.3609e-01, -4.9963e+00,\n",
       "          8.6325e+01,  3.0451e+00,  1.9086e+01,  7.6932e-01,  1.0163e+01,\n",
       "         -8.3679e-01,  9.7959e+01,  2.7549e+01,  1.7439e+01,  4.2724e+00,\n",
       "         -4.0342e+00, -2.4198e+00,  9.6469e+00,  2.3752e+00, -2.7635e+00,\n",
       "          3.1273e+00,  2.2216e+00, -1.9297e+00,  7.2305e+01,  5.3084e+00,\n",
       "          4.4520e+00, -6.4038e+00, -8.1724e+00],\n",
       "        [ 4.8260e+01,  2.4205e+01, -1.2005e-01, -1.2708e+00,  3.4238e-01,\n",
       "          1.0641e+02,  4.0890e+00,  3.9517e+01, -2.0198e+00,  5.8898e+00,\n",
       "          4.1613e+00,  3.4188e+01,  3.6472e+01,  1.4717e+01,  1.7136e+00,\n",
       "         -6.1461e+00,  1.4338e+01,  1.2871e+01,  1.2083e+01,  6.0906e-01,\n",
       "         -3.2311e-01,  2.4199e-01,  5.6087e-01,  9.2180e+01, -2.6972e+00,\n",
       "          1.4322e+00, -1.6145e+00,  1.3694e+00],\n",
       "        [ 9.0323e+01,  5.8205e+01,  2.7891e-02,  1.0781e+00, -1.4666e+00,\n",
       "          4.0909e+01,  1.8163e-01,  1.6621e+01, -1.2003e-01,  1.2795e+00,\n",
       "          4.7158e+00,  6.1931e+01,  5.6788e+01,  4.6080e+01,  6.3241e+00,\n",
       "         -1.8514e+00,  4.7872e+00, -2.8987e+00, -5.9086e-01,  4.9903e+00,\n",
       "          4.1235e-01,  3.0996e+00, -7.6710e-01,  5.5294e+00,  5.5906e+00,\n",
       "         -2.9043e+00, -1.0320e+00, -3.7108e+00],\n",
       "        [ 9.9571e+01,  7.0067e+01,  1.5563e-01, -1.3033e-01,  1.1614e+00,\n",
       "          2.7344e+01,  4.7347e-01,  1.3921e+01, -1.2636e+00,  1.9435e+00,\n",
       "          2.4052e+00,  6.9378e+01,  4.0251e+01,  4.2662e+01,  1.5015e+01,\n",
       "         -1.6255e+00,  9.5980e+00, -2.3056e+00,  9.8008e+00,  3.2490e+00,\n",
       "          2.7276e+00,  1.9672e+00,  1.3102e+00,  1.0447e+01,  8.5960e+00,\n",
       "         -3.5994e+00,  2.3605e+00, -3.1708e-01],\n",
       "        [ 8.0337e+01,  6.7130e+01,  1.1098e+00,  4.8541e-01,  2.0565e+00,\n",
       "          2.2605e+01,  3.8286e-01,  1.0776e+01,  9.0671e-01,  1.0819e+00,\n",
       "          1.0953e+00,  6.5217e+01,  2.5480e+01,  3.4050e+01,  1.7395e+01,\n",
       "         -5.2439e-01,  1.1664e+01,  3.0531e+00,  1.4297e+01, -5.4916e-01,\n",
       "          2.2038e+00,  1.6976e+00,  2.7536e+00,  1.1789e+01,  9.7056e+00,\n",
       "          3.4608e-01,  1.9280e+00,  3.4543e+00]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
